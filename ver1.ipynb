{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# from tokenizers import Tokenizer\n",
    "# from tokenizers.models import WordLevel\n",
    "# from tokenizers.trainers import WordLevelTrainer\n",
    "# from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Us humans love text (semantic) inputs and outputs, but the machine (regrettably) does not. Tokenization is the process of converting text into numerical symbols to facilitate calculations on it.\n",
    "\n",
    "The paper uses BPE (Byte-Pair Encoding). I will use the pre-trained BertTokenizer from HuggingFace Transformers Library.\n",
    "\n",
    "Bert was pretrained with two objectives:\n",
    "1. **Masked Language Modeling (MLM)**: BERT randomly masks 15% of the input tokens and trains to predict the masked words. This allows for bidirectional learning.\n",
    "2. **Next Sentence Prediction (NSP)**: Allows BERT to understand the coherence between sentences. Model has to predict if these two sentences were next to each other in the original text or not. \n",
    "\n",
    "This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks.\n",
    "\n",
    "There are a few special 'tokens' which should be noted:\n",
    "1. **`[CLS]`** (Classifier): This token is inserted at the beginning of the sequence. In NSP, this represents the entire sequence, capturing the overall meaning of the combined input.\n",
    "2. **`[SEP]`** (Separator): This token acts as a separator between the two sentences in a pair during NSP. It helps the model differentiate between the first and second sentences within the combined sequence.\n",
    "3. **`[MASK]`** (Mask): This token replaces a certain percentage of words in the input sentence. The model's objective is to predict the original masked word based on the context provided by the surrounding words.\n",
    "4. **`[UNK]`** (Unknown): Represents an unknown word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "Embedding is the conversion of any word token in to its vector representation. For example, the words 'boat' and 'ship' are closer to each other than, say, the word 'racoon'.     \n",
    "Each dimension of the vector representation nudges the entity in some semantic direction. So in the language of vectors, the cosine similarity of two entities insinuates semantic closeness.\n",
    "\n",
    "The weights for these embeddings are multiplied by \t$$\\sqrt{d_m}$$      \n",
    "Where d<sub>m</sub> == num. of dimensions of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, vocab: int, d_model: int = 512):\n",
    "        super(Embed, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab = vocab\n",
    "        self.emb = nn.Embedding(self.vocab, self.d_model)\n",
    "        self.scaling = torch.sqrt(self.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x) * self.scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "Unlike RNNs, the Transformer model has no idea of relative word positions in a sentence. For the model, the phrases    \n",
    "`Avengers beat Thanos` and `Thanos beat Avengers`    \n",
    "Mean the same, even though these two sentences are catastrophically different. \n",
    "\n",
    "Therefore, this information is injected by adding a 'positional encoding' in the input embeddings of the encoder and the decoder. The Transformer Architecture uses sine and cosine transformations to achieve this\n",
    "\n",
    "> PE<sub>(pos, 2i)</sub> = sin($pos \\over 10000^{(2i/d_m)}$)\n",
    "<br>\n",
    "\n",
    "> PE<sub>(pos, 2i+1)</sub> = cos($pos \\over 10000^{(2i/d_m)}$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int = 512, dropout: float = .1, max_len: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Computing the positional encodings in log space to avoid numerical overflow\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.Tensor([10000.0])) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "Attention mechanism is the very thing that pushed this paper to the forefront of NLP research (apparantly Attention IS all you need).    \n",
    "Here we basically do the following:\n",
    "1. Find weights by multipying (dot product) the initial embedding of the first word with the embeddings of all other words.  \n",
    "2. These weights are normalized (sum = 1)  \n",
    "3. Weights are again multiplied with the embeddings of all words  \n",
    "\n",
    "\n",
    "Attention(Q, K, V) = $softmax($ $QK^T \\over \\sqrt(d_k)$ $)V$\n",
    "\n",
    "\n",
    "In other words, each word from Query Vector ($1$ x $k$) is multiplied with the Key matrix ($k$ x $k$) and normalized. This is then multiplied with the Value Vector. We explored single head attention here.\n",
    "\n",
    "# Multi-Head Attention\n",
    "In Multi-Head Attention, we have multiple $Q, K, V$ matrices split from the input. These matrices are fed through multiple Attention Blocks. The outputs are then concatenated to give us the final Attention Output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    def __init__(self, dropout: float = 0.):\n",
    "        super(Attention, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        p_attn = self.dropout(self.softmax(scores))\n",
    "        return torch.matmul(p_attn, value)\n",
    "    \n",
    "    def __call__(self, query, key, value, mask=None):\n",
    "        return self.forward(query, key, value, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, h: int = 8, d_model: int = 512, dropout: float = 0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.attn = Attention(dropout)\n",
    "        self.lindim = (d_model, d_model)\n",
    "        self.linears = nn.ModuleList([deepcopy(nn.Linear(*self.lindim)) for _ in range(4)])\n",
    "        self.final_linear = nn.Linear(*self.lindim, bias=False)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        \n",
    "        query, key, value = [l(x).view(query.size(0), -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))]\n",
    "        nbatches = query.size(0)\n",
    "        x = self.attn(query, key, value, mask=mask)\n",
    "        \n",
    "        # Concatenate and multiply by W^O\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.final_linear(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add & Norm\n",
    "* It is known that techniques like Normalization (Mean = 0, Var = 1) and Residual Connections improve training time and performance. Hence, there is a layer of Add & Norm after every attention and feed-forward layer in both Encoder and Decoder Blocks.      \n",
    "* Residual Connections refer to adding the output of the previous layer to the current layer's output.     \n",
    "* Additionally, Dropouts are added too (help in generalization).\n",
    "\n",
    "Here, we take a residual connection of the original word embedding, add it to the embedding from the multi-head attention, and then normalize it.  \n",
    "\n",
    "Final output of each layer will then be:   \n",
    "$$ResidualConnection(x) = x+Dropout(SubLayer(LayerNorm(x)))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features: int, eps: float = 1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, size: int = 512, dropout: float = .1):\n",
    "        super(ResidualConnection,  self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Layer\n",
    "Feed-Forward networks are essential as they help provide non-linearity and complexity to the neural network. The Transformer model has a ReLU (Rectified LInear Unit) and a Dropout layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int = 512, d_ff: int = 2048, dropout: float = .1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.l1 = nn.Linear(d_model, d_ff)\n",
    "        self.l2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.l2(self.dropout(self.relu(self.l1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Block\n",
    "This block takes whole sentences as input. After the input sentence is through Input Embedding and Positional Embedding, the multi-head attention and feed-forward blocks are repeated $n$ times (hyperparameters), in the encoder block.\n",
    "\n",
    "Here, $n = 6$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size: int, self_attn: MultiHeadAttention, feed_forward: FeedForward, dropout: float = .1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sub1 = ResidualConnection(size, dropout)\n",
    "        self.sub2 = ResidualConnection(size, dropout)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sub1(x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sub2(x, self.feed_forward)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, n: int = 6):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([deepcopy(layer) for _ in range(n)])\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Block\n",
    "This block recieves two main inputs: \n",
    "* Output of previous decoder: Can be a single/ series of tokens. This will be referred to as `prev_op`.\n",
    "* Output from Encoder: Gives context\n",
    "\n",
    "The prev_op is first passed through Embedding and positional encoding. Then, a Masked Multi-Head Attention system is applied (as output value should only depend on previously fed inputs. The future is masked.)\n",
    "\n",
    "The output from Masked Multi-Head Attention layer, along with output of Encoder Block are send into a Multi-Head Attention layer. This then goes through a Feed Forward Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size: int, self_attn: MultiHeadAttention, src_attn: MultiHeadAttention, \n",
    "                 feed_forward: FeedForward, dropout: float = .1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sub1 = ResidualConnection(size, dropout)\n",
    "        self.sub2 = ResidualConnection(size, dropout)\n",
    "        self.sub3 = ResidualConnection(size, dropout)\n",
    " \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        x = self.sub1(x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sub2(x, lambda x: self.src_attn(x, memory, memory, src_mask))\n",
    "        return self.sub3(x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer: DecoderLayer, n: int = 6):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([deepcopy(layer) for _ in range(n)])\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEncoderDecoder\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoder: Encoder, decoder: Decoder, \n\u001b[1;32m      3\u001b[0m                  src_embed: Embed, tgt_embed: Embed, final_layer: Output):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m(EncoderDecoder, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m, in \u001b[0;36mEncoderDecoder\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEncoderDecoder\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoder: Encoder, decoder: Decoder, \n\u001b[0;32m----> 3\u001b[0m                  src_embed: Embed, tgt_embed: Embed, final_layer: \u001b[43mOutput\u001b[49m):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m(EncoderDecoder, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m encoder\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Output' is not defined"
     ]
    }
   ],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, \n",
    "                 src_embed: Embed, tgt_embed: Embed, final_layer: Output):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.final_layer = final_layer\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        return self.final_layer(self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask))\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output\n",
    "The vector output from the decoder has to be transformed to a final output. This is done by creating a probability distribution over the whole vocabulary for each token. A SoftMax Function is used to define the probability dictribution, but here I will use LogSoftmax as it is apparantly faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super(Output, self).__init__()\n",
    "        self.l1 = nn.Linear(input_dim, output_dim)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        op = self.l1(x)\n",
    "        return self.log_softmax(op)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
